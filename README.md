# GoGPT
> 中文底座大模型：70亿参数、130亿参数

## step1：训练分词器

## step2：二次预训练
> 在中文预训练语料上对LLaMA进行增量预训练、继续预训练
## step3: 有监督微调

## 免责声明
本项目相关资源仅供学术研究之用，严禁用于商业用途。 使用涉及第三方代码的部分时，请严格遵循相应的开源协议。

模型生成的内容受模型计算、随机性和量化精度损失等因素影响，本项目不对其准确性作出保证。

对于模型输出的任何内容，本项目不承担任何法律责任，亦不对因使用相关资源和输出结果而可能产生的任何损失承担责任。

## 